{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = \"/home/amit/Downloads/review-data/train_data.txt\"\n",
    "test_path = \"/home/amit/Downloads/review-data/test_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    ratemap = {'1.0':0,'2.0':0,'3.0':0,'4.0':1,'5.0':1,'1':1,'0':0}\n",
    "    data = open(path)\n",
    "    X, y = [], []\n",
    "    for line in data:\n",
    "        text = line.partition(\",\")\n",
    "        if text[0] in ratemap.keys():\n",
    "            X.append(text[2])\n",
    "            y.append(ratemap[text[0]])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train = load_data(train_path)\n",
    "X_test, y_test = load_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Length of data: %d\" % len(X_train)\n",
    "print \"Sample data: %s\" % X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "def tokenize_words(text,remove_stopwords=False):\n",
    "    # Remove HTML Tags\n",
    "    text = re.sub('<[^>]*>','',text)\n",
    "    # Keep Smileys\n",
    "    smileys = re.findall('((?::|;|=)(?:-?)(?:[D|d|)|(|P|p|/|x|X]))',text)\n",
    "    # Remove non words\n",
    "    text = re.sub('[\\W]+',' ',text.lower())\n",
    "    text += ' '.join(smileys).replace('-','')\n",
    "    # Remove Stopwords\n",
    "    if remove_stopwords:\n",
    "        tokenized = [w for w in text.split() if w not in stop]\n",
    "        return tokenized\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def func_tokenize_sents(review,remove_stopwords=False):\n",
    "    sentences = nltk.sent_tokenize(review)\n",
    "    tokenized_sents = []\n",
    "    for sent in sentences:\n",
    "        if len(sent) > 0:\n",
    "            tokenized_sents.append(tokenize_words(sent,remove_stopwords))\n",
    "    return tokenized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tokenized_sents(data):\n",
    "    tokenized_sents = []\n",
    "    count = 0\n",
    "    for review in data:\n",
    "        try:\n",
    "            tokenized_sents += func_tokenize_sents(review)\n",
    "        except:\n",
    "            continue\n",
    "    return tokenized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tokenized_sents = get_tokenized_sents(X_train)\n",
    "test_tokenized_sents = get_tokenized_sents(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(train_tokenized_sents)\n",
    "print len(test_tokenized_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 400 #Word Vector Dimension\n",
    "min_word_count = 30\n",
    "num_threads = 4 # Number of threads to run in parallel\n",
    "context = 10 # contet window size\n",
    "downsampling = 0.001 #downsample frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from gensim.models import word2vec\n",
    "start_time = datetime.now()\n",
    "model = word2vec.Word2Vec(train_tokenized_sents + test_tokenized_sents,\n",
    "                          workers=num_threads,\n",
    "                          size=num_features,\n",
    "                          min_count=min_word_count,\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "model.init_sims(replace=True)\n",
    "model_name = \"400features_30minwords_10context\"\n",
    "model.save(model_name)\n",
    "print \"Time take to generate word vectors: %s\" % str(datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"400features_30minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print model.doesnt_match(\"man woman child kitchen\".split())\n",
    "print model.most_similar(\"queen\")\n",
    "print model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"Type of model:\",type(model.syn0)\n",
    "print \"Shape of model array: %s\" % (str(model.syn0.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Average the feature vectors to combine the words in each review\n",
    "def avgFeatureVec(words,model,num_features):\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords=0\n",
    "    index2word = set(model.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word:\n",
    "            nwords += 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    if nwords > 0:\n",
    "        featureVec = np.divide(featureVec,nwords)\n",
    "        return featureVec\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getavgFeatureVecs(reviews,y, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = []\n",
    "    label = []\n",
    "    for i in xrange(len(reviews)):\n",
    "        temp = avgFeatureVec(reviews[i],model,num_features)\n",
    "        if temp != None:\n",
    "            reviewFeatureVecs.append(temp)\n",
    "            label.append(y[i])\n",
    "            counter += 1\n",
    "    return reviewFeatureVecs, label        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_train_data(y):\n",
    "    train_reviews = []\n",
    "    for review in X_train:\n",
    "        train_reviews.append(tokenize_words(review))\n",
    "\n",
    "    trainData,trainY = getavgFeatureVecs(train_reviews,y,model,num_features)\n",
    "    return np.asarray(trainData,dtype=theano.config.floatX),np.asarray(trainY,dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_test_data(y):\n",
    "    test_reviews = []\n",
    "    for review in X_test:\n",
    "        test_reviews.append(tokenize_words(review))\n",
    "\n",
    "    testData, testY = getavgFeatureVecs(test_reviews,y,model,num_features)\n",
    "    return np.asarray(testData,dtype=theano.config.floatX),np.asarray(testY,dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train = get_train_data()\n",
    "X_test, y_test = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Training Data Shape:\",X_train.shape,y_train.shape\n",
    "print \"Testing Data Shape:\",X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self,input,nn_input_dim,activation=T.tanh,nn_hidden_dim=100,nn_output_dim=2):\n",
    "        \n",
    "        self.input = input\n",
    "        self.nn_input_dim = nn_input_dim\n",
    "        self.nn_hidden_dim = nn_hidden_dim\n",
    "        \n",
    "        rng = np.random.RandomState(0)\n",
    "        \n",
    "        W1 = np.asarray(\n",
    "            rng.uniform(\n",
    "                low=-np.sqrt(6. / (nn_input_dim + nn_hidden_dim)),\n",
    "                high=np.sqrt(6. / (nn_input_dim + nn_hidden_dim)),\n",
    "                size=(nn_input_dim,nn_hidden_dim)\n",
    "            ),\n",
    "            dtype=theano.config.floatX\n",
    "        )\n",
    "        b1 = np.zeros((nn_hidden_dim,),dtype=theano.config.floatX)\n",
    "        W2 = np.asarray(\n",
    "            rng.uniform(\n",
    "                low=-np.sqrt(6. / (nn_output_dim + nn_hidden_dim)),\n",
    "                high=np.sqrt(6. / (nn_output_dim + nn_hidden_dim)),\n",
    "                size=(nn_hidden_dim,nn_output_dim)\n",
    "            ),\n",
    "            dtype=theano.config.floatX\n",
    "        )\n",
    "        b2 = np.zeros((nn_output_dim,),dtype=theano.config.floatX)\n",
    "        \n",
    "        self.W1 = theano.shared(name='W1',value=W1)\n",
    "        self.b1 = theano.shared(name='b1',value=b1)\n",
    "        self.W2 = theano.shared(name='W2',value=W2)\n",
    "        self.b2 = theano.shared(name='b2',value=b2)\n",
    "        \n",
    "        z1 = (self.input).dot(self.W1) + self.b1\n",
    "        a1 = activation(z1)\n",
    "        z2 = a1.dot(self.W2) + self.b2\n",
    "        \n",
    "        self.p_y_given_x = T.nnet.softmax(z2)\n",
    "        \n",
    "        self.y_pred = T.argmax(self.p_y_given_x,axis=1)\n",
    "        \n",
    "        self.params = [self.W1,self.b1,self.W2,self.b2]\n",
    "    \n",
    "    def forward_propagation(self,activation=T.tanh):\n",
    "        W1, W2 = self.W1, self.W2\n",
    "        b1, b2 = self.b1, self.b2\n",
    "        \n",
    "        z1 = self.input.dot(W1) + b1\n",
    "        a1 = activation(z1)\n",
    "        z2 = a1.dot(W2) + b2\n",
    "        y_c = T.nnet.softmax(z2)\n",
    "        \n",
    "        return y_c\n",
    "        \n",
    "    def negative_log_likelihood(self,y,reg_lambda=0.01):\n",
    "       \n",
    "        loss_reg = 1./((self.input).shape[0]) * reg_lambda/2 * T.sum((self.W1**2).sum() + (self.W2**2).sum())\n",
    "        #return T.nnet.categorical_crossentropy(self.p_y_given_x,y).mean() + loss_reg\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]),y]) + loss_reg\n",
    "    \n",
    "    def predict(self):\n",
    "        return T.argmax(self.p_y_given_x,axis=1)\n",
    "    \n",
    "    def error(self,y):\n",
    "        if self.y_pred.ndim != y.ndim:\n",
    "            raise TypeError('y and y_c shape mismatch',('y',y.type,'y_c',self.y_pred.type))\n",
    "        return T.mean(T.neq(y,self.y_pred))\n",
    "    \n",
    "    def gradient_params(self):\n",
    "        loss = self.negative_log_likelihood()\n",
    "        \n",
    "        dW1 = T.grad(loss,self.W1)\n",
    "        db1 = T.grad(loss,self.b1)\n",
    "        dW2 = T.grad(loss,self.W2)\n",
    "        db2 = T.grad(loss,self.b2)\n",
    "        \n",
    "        grad_params = [dW1,db1,dW2,db2]\n",
    "        return grad_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pydot\n",
    "from IPython.display import Image\n",
    "from IPython.display import SVG\n",
    "\n",
    "#Check if there is no NaN operation while training \n",
    "def detect_nan(i, node, fn):\n",
    "    for output in fn.outputs:\n",
    "        if (not isinstance(output[0], np.random.RandomState) and\n",
    "            np.isnan(output[0]).any()):\n",
    "            print '*** NaN detected ***'\n",
    "            theano.printing.debugprint(node)\n",
    "            print 'Inputs : %s' % [input[0] for input in fn.inputs]\n",
    "            print 'Outputs: %s' % [output[0] for output in fn.outputs]\n",
    "            break\n",
    "\n",
    "def run_nn(num_passes=1000,print_loss=False,learning_rate=0.01,batch_size=500):\n",
    "    \n",
    "    X = T.matrix('X')\n",
    "    y = T.ivector('y')\n",
    "\n",
    "    nn_model = NeuralNet(input=X,nn_input_dim=400)\n",
    "    \n",
    "    cost = nn_model.negative_log_likelihood(y)\n",
    "    \n",
    "    test_model = theano.function(\n",
    "        inputs=[X,y],\n",
    "        outputs=nn_model.error(y)\n",
    "    )\n",
    "    \n",
    "    grad_params = [T.grad(cost,param) for param in nn_model.params]\n",
    "    \n",
    "    updates = [(param, param - learning_rate * gradient) for param,gradient in zip(nn_model.params, grad_params)]\n",
    "    \n",
    "    train_model = theano.function(\n",
    "        inputs=[X,y],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        mode=theano.compile.MonitorMode(\n",
    "            post_func=detect_nan\n",
    "        ).excluding('local_elemwise_fusion', 'inplace')\n",
    "    )\n",
    "    \n",
    "    theano.printing.pydotprint(train_model,var_with_name_simple=True,compact=True,outfile='nn-theano-train.png',format='png')\n",
    "    \n",
    "    n_train_batches = X_train.shape[0] / batch_size\n",
    "    n_test_batches = X_test.shape[0] / batch_size\n",
    "    \n",
    "    validate_after = 100\n",
    "    flag = True\n",
    "    for i in xrange(num_passes):\n",
    "        cost = 0.\n",
    "        for j in xrange(n_train_batches):\n",
    "            cost += train_model(X_train[j*batch_size:(j+1)*batch_size],y_train[j*batch_size:(j+1)*batch_size])\n",
    "            if np.isnan(cost):\n",
    "                flag = False\n",
    "                break\n",
    "        if flag == False:\n",
    "            break\n",
    "        #print \"Cost after %d pass: %f\"%(i,cost/n_train_batches)\n",
    "        if i % validate_after == 0:\n",
    "            test_losses = [test_model(X_test[k*batch_size:(k+1)*batch_size],y_test[k*batch_size:(k+1)*batch_size]) for k in xrange(n_test_batches)]\n",
    "            test_score = np.mean(test_losses)\n",
    "            print \"Loss after %d iterations is %f\" % (i, test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_nn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
